<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Understanding Large Language Models</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link href="https://fonts.googleapis.com/css2?family=Merriweather:wght@400;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Merriweather', serif;
      background-color: #f9f9f9;
    }
    pre {
      background-color: #f4f4f4;
      padding: 1rem;
      border-radius: 0.5rem;
      overflow-x: auto;
      font-size: 0.95rem;
    }
    code {
      font-family: 'Courier New', monospace;
    }
    .toc a {
      display: block;
      margin-bottom: 0.5rem;
      color: #1d4ed8;
      font-weight: 500;
    }
  </style>
</head>
<body class="text-gray-800">
  <header class="bg-white shadow-md py-6 px-4 text-center border-b">
    <h1 class="text-4xl font-bold tracking-tight">Understanding Large Language Models</h1>
    <p class="text-lg mt-2 text-gray-600">An intuitive guide to how language models work — from text to tokens to Transformers</p>
  </header>

  <section class="bg-white py-10 px-4 md:px-12 lg:px-24 border-b">
    <div class="max-w-6xl mx-auto">
      <h2 class="text-2xl font-bold mb-4">Table of Contents</h2>
      <div class="grid grid-cols-1 sm:grid-cols-2 gap-x-6 gap-y-2">
        <a href="#section1" class="text-blue-600 hover:underline">1. What is a Language Model?</a>
        <a href="#section2" class="text-blue-600 hover:underline">2. From Text to Tokens</a>
        <a href="#section3" class="text-blue-600 hover:underline">3. Recurrent Neural Networks (RNNs)</a>
        <a href="#section4" class="text-blue-600 hover:underline">4. Transformers</a>
        <a href="#section5" class="text-blue-600 hover:underline">5. Large Language Models (LLMs)</a>
        <a href="#section6" class="text-blue-600 hover:underline">6. Resources</a>
      </div>
    </div>
  </section>

  <!-- Section 1 -->
  <section id="section1" class="py-16 px-6 bg-gray-50">

    <div class="max-w-6xl mx-auto">
      <h2 class="text-3xl font-bold mb-6">1. What is a Language Model?</h2>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        A language model is a type of machine learning model trained to understand and generate human language. Its main job is to predict the next word in a sentence, based on the words that came before. This prediction power enables a wide range of tasks — from autocomplete in your phone, to smart assistants like ChatGPT, to real-time translation services.
      </p>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        Instead of understanding language like humans do, language models work statistically. They’ve seen billions of examples and learn the patterns of how words usually follow one another. This is why they can respond with surprisingly fluent and relevant text — even without true understanding.
      </p>
  
      <div class="border-l-4 border-blue-600 bg-blue-50 p-4 rounded mb-6">
        <p class="font-semibold text-blue-900 mb-1">Analogy</p>
        <p class="text-gray-800">
          Think of it like the game <strong>Mad Libs</strong>. If you say: “The cat sat on the ___,” a language model fills in “mat” — not because it knows what a cat is, but because it has seen that phrase thousands of times during training.
        </p>
      </div>
  
      <p class="text-lg text-gray-700 leading-relaxed">
        Language models are the foundation of modern AI. They're what power chatbots, summarize documents, write code, translate languages, and even help diagnose medical issues — all by learning patterns in language.
      </p>
    </div>
  </section>

 <!-- Section 2 -->
  <section id="section2" class="pb-8 px-6 bg-gray-50">  
  <div class="max-w-6xl mx-auto">
    <h2 class="text-3xl font-bold mb-6">2. From Text to Tokens</h2>

    <p class="text-lg text-gray-700 leading-relaxed mb-4">
      Computers can’t understand language the way we do — they need numbers. So before any language model can learn or make predictions, it first has to convert raw text into something machine-readable: <strong>tokens</strong>.
    </p>

    <p class="text-lg text-gray-700 leading-relaxed mb-4">
      A token is simply a piece of text. It might be a word, part of a word, or even punctuation — depending on the tokenization strategy. For example, the sentence:
      <em>“Language models are powerful.”</em> could be tokenized into: [“Language”, “models”, “are”, “powerful”, “.”]
    </p>

    <p class="text-lg text-gray-700 leading-relaxed mb-4">
      Once tokenized, the model assigns each token a unique number, and these numbers are what it actually learns from — not the words themselves.
    </p>

    <div class="border-l-4 border-yellow-600 bg-yellow-50 p-4 rounded mb-6">
      <p class="font-semibold text-yellow-900 mb-1">Analogy</p>
      <p class="text-gray-800">
        Imagine a sentence is a basket of groceries. Tokenization is like listing every item in the basket. The model doesn’t care about flavors or brands — it just needs the list of ingredients to start learning.
      </p>
    </div>

    <p class="text-lg text-gray-700 leading-relaxed mb-6">
      One of the simplest ways to tokenize is the <strong>Bag of Words</strong> model — where we ignore order, and just count how many times each word appears. Here's a tiny example:
    </p>

    <pre><code># Sample text documents
docs = ["I love movies", "Movies are fun", "I love fun"]
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(docs)
print(vectorizer.get_feature_names_out())
print(X.toarray())</code></pre>

    <p class="text-lg text-gray-700 leading-relaxed mb-4">
      Each document becomes a row in a matrix, and each word becomes a column. The numbers show how often each word appears in each sentence.
    </p>

    <h3 class="text-2xl font-semibold mt-10 mb-4">Try It Yourself</h3>
    <p class="text-gray-700 mb-4">
      Type your own sentence below and see how it gets tokenized:
    </p>

    <input id="inputText" type="text" class="w-full p-3 border border-gray-300 rounded mb-4" placeholder="Type or paste a sentence here...">
    <button onclick="runTokenizer()" class="bg-blue-600 text-white px-4 py-2 rounded hover:bg-blue-700">Tokenize</button>
    <div id="tokenOutput" class="mt-6 text-gray-700 text-base leading-relaxed"></div>
  </div>
</section>

  <script>
    function runTokenizer() {
      const input = document.getElementById('inputText').value.trim();
      if (!input) return;
      const tokens = input.match(/\b\w+\b/g) || [];
      if (window.dataLayer) {
        dataLayer.push({ event: 'tokenizer_used', input_length: input.length, token_count: tokens.length });
      }
      document.getElementById('tokenOutput').innerHTML = `
        <p><strong>Token Count:</strong> ${tokens.length}</p>
        <p><strong>Tokens:</strong></p>
        <div class="bg-gray-100 p-4 rounded mt-2 whitespace-pre-wrap break-words">
          <code>${tokens.join(', ')}</code>
        </div>
      `;
    }
  </script>

  <!-- Section 3 -->
  <section id="section3" class="pb-8 px-6 bg-gray-50">
    <div class="max-w-6xl mx-auto">
      <h2 class="text-3xl font-bold mb-6">3. Recurrent Neural Networks (RNNs)</h2>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        Language has order. The meaning of a sentence often depends on the words that came before — and that's where <strong>Recurrent Neural Networks (RNNs)</strong> come in.
      </p>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        RNNs are designed to process text <em>one word at a time</em>, keeping a memory of what came before. Think of it like reading a sentence from left to right, storing what you've read so far as you go.
      </p>
  
      <div class="border-l-4 border-blue-600 bg-blue-50 p-4 rounded mb-6">
        <p class="font-semibold text-blue-900 mb-1">Analogy</p>
        <p class="text-gray-800">
          Imagine you’re listening to a story one word at a time through a tiny earpiece. You don’t get to rewind — you just try to remember as much as possible while the words keep coming.
        </p>
      </div>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        This works well for short sequences, but there's a problem: as the sentence gets longer, RNNs tend to <strong>forget the earlier words</strong>. The memory fades, and the model becomes less accurate over time.
      </p>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        To fix this, researchers introduced two smarter versions of RNNs:
      </p>
  
      <ul class="list-disc pl-6 text-gray-700 space-y-2 mb-6">
        <li><strong>LSTM (Long Short-Term Memory)</strong>: adds a mechanism that helps the network remember important information for longer and decide what to forget.</li>
        <li><strong>GRU (Gated Recurrent Unit)</strong>: a simpler, faster version of LSTM that still helps with long-term memory.</li>
      </ul>
  
      <p class="text-lg text-gray-700 leading-relaxed">
        These models significantly improved sequence learning, but they still processed words one by one — making them slow and limited in how much context they could handle at once. That led to the next breakthrough: the Transformer.
      </p>
    </div>
  </section>

  <!-- Section 4 -->
  <section id="section4" class="pb-8 px-6 bg-gray-50">
    <div class="max-w-6xl mx-auto">
      <h2 class="text-3xl font-bold mb-6">4. Transformers</h2>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        Transformers completely changed how machines understand language. Unlike RNNs that read one word at a time, Transformers look at the <strong>entire sentence all at once</strong>. This parallel processing makes them faster, more scalable, and more accurate — especially for long texts.
      </p>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        The key idea behind Transformers is <strong>self-attention</strong>. This allows the model to decide which words in a sentence are most relevant to each other — regardless of their position.
      </p>
  
      <div class="border-l-4 border-purple-600 bg-purple-50 p-4 rounded mb-6">
        <p class="font-semibold text-purple-900 mb-1">Analogy</p>
        <p class="text-gray-800">
          Imagine reading an entire paragraph and being able to instantly highlight the most important words that explain the meaning of each sentence. That’s what self-attention does — it lets the model “focus” on what matters.
        </p>
      </div>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        This architecture is what powers models like BERT, GPT, and even me. It’s why Transformers are now the foundation of nearly all modern language AI — from search engines to chatbots.
      </p>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-0">
        Transformers are not just faster — they’re smarter. They can consider every word’s relationship to every other word, all at once, which leads to richer understanding and more fluent generation.
      </p>
    </div>
  </section>
  <!-- Section 5 -->
  <section id="section5" class="pb-8 px-6 bg-gray-50">
    
    <div class="max-w-6xl mx-auto">
      <h2 class="text-3xl font-bold mb-6">5. Large Language Models (LLMs)</h2>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        Large Language Models — or LLMs — are the next step in the evolution of language AI. They’re built on the Transformer architecture, but scaled up dramatically: more data, more parameters, more compute.
      </p>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        Models like GPT-3.5, GPT-4, Claude, and PaLM have been trained on vast amounts of text from books, websites, and conversations. Because of this, they can write essays, answer questions, generate code, create summaries, hold conversations, and much more.
      </p>
  
      <div class="border-l-4 border-green-600 bg-green-50 p-4 rounded mb-6">
        <p class="font-semibold text-green-900 mb-1">Analogy</p>
        <p class="text-gray-800">
          Imagine giving a student access to the entire internet and letting them read non-stop for years. They won’t remember everything exactly, but they'll be able to guess what makes sense in almost any situation.
        </p>
      </div>
  
      <p class="text-lg text-gray-700 leading-relaxed mb-4">
        LLMs don’t “think” or “understand” the way humans do — they generate the most statistically likely response based on the input they’re given. But with enough data and scale, that statistical guesswork starts to feel surprisingly intelligent.
      </p>
  
      <p class="text-lg text-gray-700 leading-relaxed">
        As powerful as they are, LLMs have limitations: they can hallucinate facts, reflect biases in their training data, and can’t access real-time information unless specifically designed to. But their capabilities today — and potential tomorrow — make them one of the most transformative technologies in AI.
      </p>
    </div>
  </section>

  <!-- Section 6 -->
  <section id="section6" class="pb-8 px-6 bg-gray-50">
    
    <div class="max-w-6xl mx-auto">
      <h2 class="text-3xl font-bold mb-6">6. Resources</h2>
      <ul class="list-disc pl-6 space-y-3 text-lg text-gray-700">
        <li><a class="text-blue-600 underline" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown – Deep Learning Playlist</a></li>
        <li><a class="text-blue-600 underline" href="https://huggingface.co/learn">Hugging Face Learning Hub</a></li>
        <li><a class="text-blue-600 underline" href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer by Jay Alammar</a></li>
      </ul>
    </div>
  </section>

  <footer class="text-center text-sm text-gray-500 py-6 border-t mt-10">
    © 2025 Understanding LLMs. Built with care.
  </footer>
</body>
</html>
