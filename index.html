<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM Explained</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap" rel="stylesheet">
  <style>
    body { font-family: 'Poppins', sans-serif; }
    pre { background: #f5f5f5; padding: 1rem; border-radius: 0.5rem; overflow-x: auto; }
    code { font-family: 'Courier New', monospace; }
  </style>
</head>
<body class="bg-white text-gray-800">
  <header class="bg-gray-100 shadow-md py-6 px-4 text-center">
    <h1 class="text-4xl font-bold">LLM Explained</h1>
    <p class="text-lg mt-2">A Deep Dive into Language Models, from Bag-of-Words to Transformers and Beyond</p>
  </header>

  <nav class="sticky top-0 z-10 bg-white border-b px-4 py-2 text-sm shadow-sm">
    <a href="#section1" class="mr-4 hover:underline">Language Modeling Basics</a>
    <a href="#section2" class="mr-4 hover:underline">RNNs</a>
    <a href="#section3" class="mr-4 hover:underline">Transformers</a>
    <a href="#section4" class="mr-4 hover:underline">LLMs</a>
    <a href="#section5" class="hover:underline">Resources</a>
  </nav>

  <!-- Section 1 -->
  <section id="section1" class="section px-6">
    <h2 class="text-2xl font-semibold mb-6">1. Language Modeling Basics</h2>

    <!-- Subsection 1.1 -->
    <div class="mb-10">
      <h3 class="text-xl font-semibold mb-2">1.1 What is a Language Model?</h3>
      <p class="mb-4">A language model is a type of machine learning model that predicts the next word in a sequence. It's like predictive text on your phone ‚Äî given some text, it tries to guess what comes next. Language models are foundational to systems like ChatGPT, autocomplete, translation engines, and more.</p>

      <div class="bg-blue-50 border-l-4 border-blue-400 p-4 rounded mb-4">
        <p class="font-semibold">üß† Analogy:</p>
        <p>Think of a language model like a super-smart guesser playing Mad Libs. If you say "The cat sat on the ___", it has seen so many sentences that it can confidently guess "mat" ‚Äî not because it understands, but because it‚Äôs statistically the most likely word.</p>
      </div>

      <div class="mb-4">
        <p class="font-medium mb-2">üé• Watch:</p>
        <iframe width="100%" height="315" src="https://www.youtube.com/embed/LPZh9BOjkQs" title="YouTube video player" frameborder="0" allowfullscreen class="rounded"></iframe>
      </div>

      <div class="mb-4">
        <p class="font-medium mb-2">üíª Example Code:</p>
        <pre><code># Basic n-gram-like example in Python
text = "the cat sat on the mat"
tokens = text.split()

# Predict the next word given a bigram
def predict_next(tokens, prev_word):
    for i in range(len(tokens)-1):
        if tokens[i] == prev_word:
            return tokens[i+1]
    return None

print(predict_next(tokens, "the"))  # Output: cat or mat</code></pre>
      </div>

      <!-- üñºÔ∏è Optional Visual Placeholder -->
      <p class="text-gray-500 italic">[Diagram of word prediction in a sentence goes here]</p>
    </div>
  </section>

  <!-- Section 2 -->
  <section id="section2" class="section bg-gray-50 px-6">
    <h2 class="text-2xl font-semibold mb-4">2. Recurrent Neural Networks</h2>
    <ul class="list-disc ml-6 space-y-2">
      <li><strong>2.1</strong> What is an RNN?</li>
      <li><strong>2.2</strong> Mini-Batch Gradient Descent</li>
      <li><strong>2.3</strong> Programming an RNN</li>
      <li><strong>2.4</strong> RNN as a Language Model</li>
      <li><strong>2.5</strong> Embedding Layer</li>
      <li><strong>2.6</strong> Training & Loss</li>
    </ul>
  </section>

  <!-- Section 3 -->
  <section id="section3" class="section px-6">
    <h2 class="text-2xl font-semibold mb-4">3. Transformers</h2>
    <ul class="list-disc ml-6 space-y-2">
      <li><strong>3.1</strong> Decoder Block</li>
      <li><strong>3.2</strong> Self-Attention</li>
      <li><strong>3.3</strong> Position Embeddings</li>
      <li><strong>3.4</strong> Multi-Head Attention</li>
      <li><strong>3.5</strong> Normalization Techniques</li>
      <li><strong>3.6</strong> Transformer in Python</li>
    </ul>
  </section>

  <!-- Section 4 -->
  <section id="section4" class="section bg-gray-50 px-6">
    <h2 class="text-2xl font-semibold mb-4">4. Large Language Models</h2>
    <ul class="list-disc ml-6 space-y-2">
      <li><strong>4.1</strong> Why Bigger is Better</li>
      <li><strong>4.2</strong> Supervised Finetuning</li>
      <li><strong>4.3</strong> Sampling Techniques</li>
      <li><strong>4.4</strong> Low-Rank Adaptation (LoRA)</li>
      <li><strong>4.5</strong> Prompt Engineering</li>
      <li><strong>4.6</strong> Ethics & Hallucinations</li>
    </ul>
  </section>

  <!-- Section 5 -->
  <section id="section5" class="section px-6">
    <h2 class="text-2xl font-semibold mb-4">5. Resources</h2>
    <p class="mb-4">Hand-picked videos, code demos, and papers will be added here.</p>
    <ul class="list-disc ml-6 space-y-2">
      <li>YouTube Lectures (to be embedded)</li>
      <li>Research Paper Links</li>
      <li>Open Source LLM Tools</li>
    </ul>
  </section>

  <footer class="text-center text-sm text-gray-500 py-6 border-t mt-10">
    ¬© 2025 LLM Explained. Built with ‚ù§Ô∏è
  </footer>
</body>
</html>
